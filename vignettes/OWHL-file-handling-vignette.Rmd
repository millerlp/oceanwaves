---
title: "OWHL file handling"
author: "Luke Miller"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{OWHL file handling}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

The `oceanwaves` package was originally created to support the post-processing workflow of the Open Wave Height Logger project. The package contains functions to assist in combining raw csv files from a OWHL device, and this vignette contains a walkthrough of the steps involved in proceeding from raw pressure data contained in an OWHL csv file to a time series of sea surface elevation data that can be used to generate oecan wave statistics. 

# Starting out

Begin by installing the `oceanwaves` package, if not already installed. The `oceanwaves` package makes use of several other packages that may also need to be 
installed.

```{r install, eval=FALSE}
install.packages('oceanwaves')
```

Load the installed package.

```{r setup}
library(oceanwaves)
```

# Importing OWHL csv files

The Open Wave Height Logger produces daily datafiles, in a comma-separated-value (.csv) format. Those files contain a header
row with optional deployment information, along with a row of column headers. The import process would normally begin by generating a 
vector of filenames (include file path) that you want to join. The
`dir()` function can be used to generate the set of filenames.

```{r, echo=FALSE}
filenames = c(system.file("extdata","20160819_1820_00.csv",
                          package = "oceanwaves"), 
              system.file("extdata","20160819_1900_00.csv", 
                          package = "oceanwaves"))
              
# filenames <- dir(path=, pattern = '*.csv', full.names=TRUE)
```

```{r getFilenames, eval=FALSE}
filenames <- dir(path=".", pattern = '*.csv', full.names=TRUE)
```

The set of filenames can be passed to the `joinOWHLfiles` function. 
This function has an argument `timezone` (default `timezone = UTC`) used
to set the time zone of the time stamps contained in the OWHL data files. If the clock of your OWHL was set to a time zone other than UTC, then supply the appropriate 
time zone name for your clock. For example, if your OWHL clock was
instead set to Pacific Standard Time during the deployment, you would
set the argument as `timezone = 'etc/GMT+8'`. Verbose output for the
file import can be turned on with the `verbose = TRUE` argument.

```{r joinOWHLfiles}
dat = joinOWHLfiles(filenames, timezone = 'UTC',verbose = TRUE)
```

The `joinOWHLfiles` function consists of several steps:

* Grab mission info (if present) from the header of the 1st csv file. This assumes that mission info is the same in all input files, which is the case if all files came from a single deployment of the OWHL.
* Read in each csv file in a loop. 
* The human-readable timestamp data in a csv file's DateTime column are converted to POSIXct time format with the desired time zone. 
* The DateTime values then have the fractional seconds appended based on the values in the `frac.seconds` column of the csv file.
* Concatenates the data from all input csv files and reorders them based on the DateTime column
* Any suspect time values (based on the `frac.seconds` column values) are filtered and removed from the data set. 
* Any 1 second gaps in the data set are filled by linear interpolation. These gaps arise in the raw data files when a micro SD card takes longer than the sample interval (default 0.25 seconds) to carry out its internal housekeeping routines. Longer gaps are assumed to be due to other issues and are not filled. The pressure data and temperature data
are filled by linear interpolation from the values immediately before and after the missing second of data. 

After carrying out these steps, a data frame is returned. The 
data frame should contain the same columns as the original input csv files. 

```{r headnewdata}
head(dat)
```


# Converting pressure to sea surface elevation

## Removing atmospheric pressure and sensor offset

The `pressure.mbar` values in the resulting data frame are absolute
pressures in millibar. This includes the pressure signal due to 
the air above the sea surface, which we will need to remove from the
data in order to get only the pressure due to the column of water above the OWHL sensor. The sea level atmospheric pressure could come from a variety of sources including nearby weather stations. Ideally this would be a time series of atmospheric pressure records that aligns with the 
OWHL time series. For this example, we will just use a single average
value for the entire time series based on the pressure reading at the start of the deployment of 1014 mbar. 

```{r atmosPressure}
atmosPressure.mbar = 1014
dat$swPressure.mbar = dat$Pressure.mbar - atmosPressure.mbar
```

It is also important to judge whether the OWHL sensor had any 
pressure offset due to factors such as manufacturing variance or 
residual pressure in the oil-filled bladder attached to the sensor (if
used). One method to remove any offset is to compare a set of pressure
readings from the data set while the OWHL was at the sea surface just prior deployment underwater. Ideally the value recorded by the OWHL at the ocean surface would be equivalent to the local atmospheric pressure, but any difference that exists can be removed from the entire timeseries. 

Here we'll define a time value when the OWHL was known to be at the ocean surface, in the boat, just before being taken underwater. Then we
find the row index in the `dat` data frame that is closest to that
time stamp, and extract 10 seconds of data (40 samples at 4 Hz), and calculate the average pressure reading. This assumes that you have 
included the data from the surface period prior to the dpeloyment, which we will remove at a later step. 

```{r surfaceTime}
# Define a surface reading for OWHL and start of subsurface deployment
owhlSurfaceTime = as.POSIXct('2016-08-19 18:25', tz = 'UTC')

# Grab an average surface pressure reading for the OWHL unit
surfaceIndx = which.min(abs(dat$DateTime - owhlSurfaceTime))

# Grab 10 seconds of surface pressure readings. This will be used to determine
# any pressure offset in the sensor, when compared to local sea surface air
# pressure at the time of deployment.
surfaceOWHLPress = dat$Pressure.mbar[surfaceIndx:(surfaceIndx+40)]
surfaceOWHLPress = round(mean(surfaceOWHLPress),dig=1)

# Calculate the offset from the known sea level atmospheric pressure
# (usally obtained from a weather station)
offsetValue = surfaceOWHLPress - atmosPressure.mbar

# Subtract offsetValue from all pressure values
dat$swPressure.mbar = dat$swPressure.mbar - offsetValue

```
After subtracting off the atmospheric pressure and any offset from the absolute pressure values in the raw data files, we are left with the gauge pressure, the pressure due only to the water column above the pressure sensor (with the caveat that strong currents could impinge
on a sensor port strongly enough to induce a small pressure signal that isn't due 
purely to the pressure head above the sensor).



## Subset the subsurface portion of the dataset

Now would be a reasonable time to subset out only the data
from the underwater portion of the deployment, if there are pre- and
post-deployment data currently in the `dat` data frame. Define a 
set of time stamps that mark when the OWHL was securely mounted in its
fixture near the seafloor. Then use those time stamps to remove
data before or after the deployment. 

```{r deployTimes}
# Create timestamps when the sensor was in place on the seafloor
owhlDeployStartTime = as.POSIXct('2016-08-19 18:58', tz = 'UTC')
owhlDeployEndTime = as.POSIXct('2016-09-27 19:10', tz = 'UTC')

# Remove all surface data prior to deployment and after end
dat = dat[dat$DateTime >= owhlDeployStartTime,]
dat = dat[dat$DateTime <= owhlDeployEndTime,]
```

## Convert pressure to sea surface elevation

To convert pressure values to sea surface elevation (height of the
water column above the sensor), we'll use the `swDepth` function from the `oce` package. This function assumes that the input pressure data
are in units of decibar, not millibar, so we start by converting our 
data to decibar (just divide by 100). 

```{r}
# Convert pressure to decibar so the oce package swDepth function can be used
dat$swPressure.dbar = dat$swPressure.mbar / 100

mylatitude = 33.72 # Los Angeles
# Use the oce function swDepth to estimate the height of seawater above the
# sensor.
dat$swDepth.m = oce::swDepth(dat$swPressure.dbar, latitude = mylatitude)

```


## Correcting for pressure attenuation

If your data were produced by a pressure transducer data logger mounted near the sea floor, rather than a surface buoy, the dynamic pressure signal that the data logger records will be muted to some degree depending on the depth of the data logger and the wave period. As a result, the sea surface elevation record from the bottom-mounted sensor will be an underestimate of what the actual height of the water column above the sensor was, and this underestimate gets worse as the depth of the sensor increases or the wave period gets shorter. The `oceanwaves` package provides a function `prCorr` that attempts to correct for this pressure signal attenuation and thus better recreate the actual water surface height fluctuations (a.k.a. waves).  

The `prCorr` function needs a few arguments to work. In addition to the vector of sea surface elevations (in meters), you also need to provide the sampling frequency for the data logger (4 Hz in our example dataset), and the height of the data logger above the sea floor in 
units of meters. 


```{r pressCorr}
dat$swDepthCorrected.m = prCorr(dat$swDepth.m, Fs = 4, zpt = 0.1)
```

We'll plot the corrected data and the uncorrected data on the same plot to show how the correction changes the estimated sea surface heights. 

```{r deattenuationFigure1, fig.width = 8, fig.height = 4}
# Plot the de-attenuated data
plot(x = dat$DateTime, 
     y = dat$swDepthCorrected.m, type = 'l', col = 'red', 
     ylab = 'Surface elevation, m', xlab = 'Time')
lines(x = dat$DateTime, 
      y = dat$swDepth.m, col = 'blue') # Add the original data
legend('topright',legend=c('Corrected','Raw'), col = c('red','blue'),
       lty = 1)
```

At this point, the de-attenuated sea surface elevation data in the
`dat$swDepthCorrected.m` column can be used to calculate various
wave statistics. 

## Calculating wave statistics

Wave statistics are generally calculated for chunks of time, often in the range of 15 to 30 minutes. If the original data were collected in 
discrete chunks of time (such as a 15 minute burst followed by 45 minutes of no data collection) then the time series in `dat` will already have natural boundaries for each burst to be analyzed. If instead the data
were collected continuously, it would be necessary to create our own boundaries for each chunk of time to be analyzed. Our example data set in this vignette is short, so we will just analyze the data in 5 minute chunks.

```{r}
# Specify the sampling rate for this data set
Fs = 4 # Sampling rate, Hz

# Search for existing natural breaks in the data set. Use the
# sampling rate to determine where longer breaks exist
bounds = which(diff(dat$DateTime) > (1/Fs))

if (length(bounds) > 0){
  # If there are natural breaks in the time series, use the 
  # values in bounds 
  bounds = c(0,bounds) # Add on the first row index as well
  # We use 0 rather than 1 here to accomodate processing in the 
  # code below
} else if (length(bounds) == 0){
  # If bounds has length 0, the dataset is continuous
  # Define how long you want each analyzed time chunk to be
  timeChunk = 5 # units of minutes
  # Calculate the number of rows in a time chunk
  stepSize = timeChunk * 60 * Fs #  minutes x 60 seconds x sample rate
  # Generate a set of chunk boundaries based on stepSize
  bounds = seq(from = 0, to = nrow(dat), by = stepSize)
} 
```

After defining a vector `bounds` containing row indices in the 
`dat` data frame to be analyzed as separate time chunks, step through the time chunks.

```{r processTimeChunks}
# Go through each time chunk and calculate the wave
# height and period of the de-trended data (removing tide signal).

for (i in 2:length(bounds)){
  # Extract the chunk of contiguous data
  tempdat = dat[((bounds[i-1])+1):bounds[i],]
  # Extract the date & time at the end of the chunk
  tempDateTime = tempdat[nrow(tempdat),'DateTime']
  
  # Using the zero-crossing function to estimate sig. wave height. 
  zcstats = oceanwaves::waveStatsZC(tempdat$swDepthCorrected.m, 
                                    Fs = Fs)
  # Extract the period for the highest 1/3 of the waves
  Period.zerocross = zcstats$Tsig
  # Extract the significant wave height 
  # (i.e. mean height of highest 1/3 of waves)
  Hsig.zerocross = zcstats$Hsig
  ################
  # Now use the spectral analysis method instead
  spstats = oceanwaves::waveStatsSP(tempdat$swDepthCorrected.m, 
                                    Fs = Fs)
  # Extract the significant wave height estimate
  Hsig.spectral = spstats$Hm0 
  # Extract the peak period estimate
  Period.spectral = spstats$Tp
  
  # Stick the wave heights and periods in an output data frame 'results'
  if (i == 2){
    # Create the data frame on the first loop 
    results = data.frame(DateTime = tempDateTime,
                         Hm0.m = Hsig.spectral,
                         Tpeak.s = Period.spectral,
                         Hsig.m = Hsig.zerocross,
                         Tsig.s = Period.zerocross)
    # Add DateTime to the zerocross results
    # TODO: Fix timezone nonsense
    zcstats[['DateTime']] = as.POSIXct(tempDateTime,tz='UTC')
    # Save zerocross results as a list
    zcstatsList = zcstats 
    
    # Add DateTime to the spectral results 
    spstats[['DateTime']] = tempDateTime
    # Save spectral results as a list
    spstatsList = spstats
  } else {
    # Add DateTime to zerocross results
    zcstats[['DateTime']] = as.POSIXct(tempDateTime,tz='UTC')
    # Add to the zerocross list
    zcstatsList = mapply(c, zcstatsList, zcstats, SIMPLIFY=FALSE)
    # Add DateTime to the spectral results 
    spstats[['DateTime']] = tempDateTime
    spstatsList = mapply(c, spstatsList, spstats, SIMPLIFY=FALSE)
    
    # Extract results for this loop
    tempresults = data.frame(DateTime = tempDateTime,
                             Hm0.m = Hsig.spectral,
                             Tpeak.s = Period.spectral,
                             Hsig.m = Hsig.zerocross,
                             Tsig.s = Period.zerocross)
    # Append results to the results data frame
    results = rbind(results,tempresults)
  }
}
```

The results from the analysis are shown below. Two different estimates
of wave height and period are shown from the two different analysis
methods. `Hm0.m` is significant wave height, in meters, and `Tpeak.s` is 
peak period, in seconds, based on the spectral analysis method, while `Hsig.m` is significant wave height, in meters, of the highest 1/3 of the waves and `Tsig.s` is the period, in seconds, of the highest 1/3 of the waves based on the wave-by-wave analysis of the zero-crossing method.  

```{r waveResults}
results
```

Wave climate estimates based on spectral analysis methods are probably the more commonly accepted method currently, and the wave-by-wave analysis using the zero-crossing method is provided for interested users. 


```{r}
oceanwaves::waveStatsSP(tempdat$swDepthCorrected.m, 
                                    Fs = Fs, plot=TRUE)
```